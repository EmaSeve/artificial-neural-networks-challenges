{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1FdqcMXk96rS","zfehjCy896Fd","KUTYqJ2o8MU1","4C8TnH-ssIPA","7xxecHoWM4gk","_GJ9mpGJWZfo","Eb9DVpuR9gBd","6jpoBYs_9ihm","0fMAuMfqTVpv","aQ41S9bBjv8K"],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1FdqcMXk96rS"},"source":["## üåê **Google Drive and Kaggle Connection**"]},{"cell_type":"code","source":["# Install and configure Kaggle API\n","!pip install -q kaggle\n","\n","import os\n","from google.colab import drive\n","from google.colab import files\n","\n","# --- Configuration ---\n","# Percorso di destinazione sul Drive\n","DRIVE_MOUNT_PATH = \"/content/gdrive\"\n","DATASET_PATH = f\"{DRIVE_MOUNT_PATH}/MyDrive/Artificial_Neural_Networks/Images_Classification_Challenge/dataset\"\n","COMPETITION_ID = \"an2dl2526c2v2\"\n","ZIP_FILENAME = f\"{COMPETITION_ID}.zip\"\n","EXPECTED_ZIP_FILE = os.path.join(DATASET_PATH, ZIP_FILENAME)\n","\n","# --- 1. Mount Google Drive ---\n","print(\"1. Mounting Google Drive...\")\n","# Mount G-Drive if it's not already mounted\n","if not os.path.exists(DRIVE_MOUNT_PATH):\n","    drive.mount(DRIVE_MOUNT_PATH)\n","else:\n","    print(\"Drive already mounted.\")\n","\n","# --- 2. Check for Existing Data ---\n","if os.path.exists(EXPECTED_ZIP_FILE):\n","    print(f\"\\n‚úÖ Dataset found at {DATASET_PATH}. Skipping download and setup.\")\n","    # You can also add a check here for the unzipped folders if you prefer.\n","else:\n","    # --- 3. Setup Kaggle Credentials (Only if download is needed) ---\n","    print(\"\\n‚è≥ Dataset not found. Starting Kaggle setup and download.\")\n","\n","    # 3a. Upload kaggle.json\n","    print(\"Carica il file kaggle.json (scaricabile dal tuo profilo Kaggle)\")\n","\n","    # Check if files.upload() returned a file (if running interactively)\n","    # The uploaded dictionary keys are the filenames.\n","    uploaded = files.upload()\n","\n","    if \"kaggle.json\" in uploaded:\n","        # 3b. Configura le credenziali\n","        !mkdir -p ~/.kaggle\n","        !mv kaggle.json ~/.kaggle/\n","        !chmod 600 ~/.kaggle/kaggle.json\n","        print(\"Kaggle credentials configured.\")\n","\n","        # 3c. Create destination folder and Download\n","        !mkdir -p {DATASET_PATH}\n","        print(f\"Downloading dataset to: {DATASET_PATH}\")\n","        # Scarica il dataset direttamente da Kaggle nella cartella scelta\n","        !kaggle competitions download -c {COMPETITION_ID} -p {DATASET_PATH}\n","\n","    else:\n","        print(\"\\n‚ö†Ô∏è kaggle.json not uploaded. Cannot proceed with download.\")\n","\n","\n","# --- 4. Decompress Data (Always check for existence before unzipping) ---\n","\n","EXPECTED_UNZIPPED_FILE = os.path.join(DATASET_PATH, \"train_labels.csv\")\n","\n","if os.path.exists(EXPECTED_UNZIPPED_FILE):\n","    # Check if the key unzipped file is already there\n","    print(f\"\\nüì¶ Data appears to be already unzipped (found: {os.path.basename(EXPECTED_UNZIPPED_FILE)}). Skipping decompression.\")\n","\n","elif os.path.exists(EXPECTED_ZIP_FILE):\n","    # Only unzip if the zip file is present AND the unzipped files are missing\n","    print(\"\\nüì¶ Competition zip found but data not yet extracted. Starting decompression...\")\n","\n","    # -o flag is usually kept for safety, but if you want STRICTLY NO overwrite, you can remove it.\n","    # For speed optimization, we rely on the outer 'if' block to skip the entire step.\n","    !unzip -o {EXPECTED_ZIP_FILE} -d {DATASET_PATH}\n","\n","else:\n","    # This scenario means neither the zip nor the unzipped files were found.\n","    # This should only happen if the preceding download step failed or was skipped.\n","    print(\"\\n‚ö†Ô∏è Cannot decompress: Competition zip file is missing.\")\n","\n","print(f\"\\nFinal status: Dataset available in: {DATASET_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"XFOlOh32ZvRg","outputId":"71cfa59e-8a63-4205-a58f-38802df0192b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1. Mounting Google Drive...\n","Mounted at /content/gdrive\n","\n","‚úÖ Dataset found at /content/gdrive/MyDrive/Artificial_Neural_Networks/Images_Classification_Challenge/dataset. Skipping download and setup.\n","\n","üì¶ Data appears to be already unzipped (found: train_labels.csv). Skipping decompression.\n","\n","Final status: Dataset available in: /content/gdrive/MyDrive/Artificial_Neural_Networks/Images_Classification_Challenge/dataset\n"]}]},{"cell_type":"markdown","metadata":{"id":"zfehjCy896Fd"},"source":["## ‚öôÔ∏è **Libraries Import**"]},{"cell_type":"code","source":["# Set seed for reproducibility\n","SEED = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# Import PyTorch\n","import torch\n","torch.manual_seed(SEED)\n","from torch import nn\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision\n","from torchvision.transforms import v2 as transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchvision.datasets import OxfordIIITPet\n","from torchvision.transforms import InterpolationMode\n","!pip install torchview\n","from torchview import draw_graph\n","\n","# Configurazione di TensorBoard e directory\n","logs_dir = \"tensorboard\"\n","!pkill -f tensorboard\n","%load_ext tensorboard\n","!mkdir -p models\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.benchmark = True\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Device: {device}\")\n","\n","# Import other libraries\n","import requests\n","from io import BytesIO\n","import cv2\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"],"metadata":{"id":"NyWck-7y7nAJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c13c9ca3-a0bb-4e5d-9f39-ac7ce099ef96","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchview\n","  Downloading torchview-0.2.7-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchview) (0.21)\n","Downloading torchview-0.2.7-py3-none-any.whl (26 kB)\n","Installing collected packages: torchview\n","Successfully installed torchview-0.2.7\n","PyTorch version: 2.9.0+cu126\n","Device: cuda\n"]}]},{"cell_type":"markdown","source":["## ‚è≥ **Data Loading**"],"metadata":{"id":"KUTYqJ2o8MU1"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","def delete_samples_from_list(txt_file, img_dir, labels_csv):\n","    \"\"\"\n","    - Reads IDs from the txt file\n","    # - Converts them into file names like img_XXXX.png\n","    - Deletes img_XXXX.png and mask_XXXX.png\n","    - Removes the row in the CSV where sample_index == img_XXXX.png\n","    \"\"\"\n","\n","    # --- 1. Load IDs from the file (without modification)\n","    with open(txt_file, \"r\") as f:\n","        ids_raw = [line.strip() for line in f if line.strip()]\n","\n","    print(f\"IDs to be deleted found: {len(ids_raw)}\")\n","\n","    # --- 2. Create the complete file names as they appear in the CSV\n","    filenames = [f\"img_{idx}.png\" for idx in ids_raw]\n","\n","    # --- 3. Delete images and masks\n","    removed_files = []\n","\n","    for fname in filenames:\n","        img_path  = os.path.join(img_dir, fname)\n","        mask_path = os.path.join(img_dir, fname.replace(\"img_\", \"mask_\"))\n","\n","        for path in (img_path, mask_path):\n","            if os.path.exists(path):\n","                os.remove(path)\n","                removed_files.append(path)\n","\n","    print(f\"üóëÔ∏è Deleted {len(removed_files)} image/mask files.\")\n","\n","    # --- 4. Delete the labels from the CSV\n","    df = pd.read_csv(labels_csv)\n","\n","    # Ensure the column is a string type\n","    df[\"sample_index\"] = df[\"sample_index\"].astype(str)\n","\n","    # Filter out rows where 'sample_index' is in the list of 'filenames'\n","    df_new = df[~df[\"sample_index\"].isin(filenames)]\n","\n","    # Save the new DataFrame back to the CSV\n","    df_new.to_csv(labels_csv, index=False)\n","\n","    print(f\"üìÑ Remaining labels saved: {len(df_new)}\")\n","    print(\"‚úÖ Cleanup completed.\")"],"metadata":{"id":"mgove3UJA2pa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","# Define the final directories containing the images/masks\n","# These are the directories created by the unzipping above.\n","train_img_dir = os.path.join(DATASET_PATH, \"train_data\")\n","test_img_dir = os.path.join(DATASET_PATH, \"test_data\")\n","labels_file = os.path.join(DATASET_PATH, \"train_labels.csv\")\n","\n","# Delete Shrek ans splash images\n","delete_path = os.path.join(DATASET_PATH, \"delete.txt\")\n","delete_samples_from_list(\n","     txt_file=delete_path,\n","     img_dir=train_img_dir,\n","     labels_csv=labels_file\n",")\n","\n","# --- Load Labels and Map Classes ---\n","\n","# Load the labels file\n","labels_df = pd.read_csv(labels_file)\n","\n","# The classes are string labels (e.g., 'HER2(+)', 'Luminal B'). We need to map them to integers.\n","# This also ensures we get the ordered list of class names.\n","le = LabelEncoder()\n","labels_df['label_encoded'] = le.fit_transform(labels_df['label'])\n","\n","# Store the class names and mapping\n","class_names = list(le.classes_)\n","num_classes = len(class_names)\n","class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n","\n","print(f\"\\nFound {len(labels_df)} training samples.\")\n","print(f\"Number of classes: {num_classes}\")\n","print(f\"Class Names: {class_names}\")\n","print(f\"Label Mapping: {class_mapping}\")\n","print(labels_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"I-3T66Vwrb3Q","outputId":"534580ca-c382-4930-d369-3be195f42094"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IDs to be deleted found: 111\n","üóëÔ∏è Deleted 0 image/mask files.\n","üìÑ Remaining labels saved: 581\n","‚úÖ Cleanup completed.\n","\n","Found 581 training samples.\n","Number of classes: 4\n","Class Names: ['HER2(+)', 'Luminal A', 'Luminal B', 'Triple negative']\n","Label Mapping: {'HER2(+)': np.int64(0), 'Luminal A': np.int64(1), 'Luminal B': np.int64(2), 'Triple negative': np.int64(3)}\n","     sample_index            label  label_encoded\n","0    img_0000.png  Triple negative              3\n","1    img_0002.png        Luminal B              2\n","2    img_0003.png        Luminal B              2\n","3    img_0004.png        Luminal B              2\n","4    img_0006.png        Luminal A              1\n","..            ...              ...            ...\n","576  img_0686.png  Triple negative              3\n","577  img_0687.png  Triple negative              3\n","578  img_0688.png        Luminal A              1\n","579  img_0689.png        Luminal A              1\n","580  img_0690.png        Luminal A              1\n","\n","[581 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["# --- Class Distribution ---\n","\n","class_counts = labels_df['label'].value_counts().sort_index()\n","class_percent = labels_df['label'].value_counts(normalize=True).sort_index() * 100\n","\n","print(\"\\n=== Class Distribution ===\")\n","for cls in class_counts.index:\n","    print(f\"{cls:15}  Count: {class_counts[cls]:4d}   ({class_percent[cls]:5.2f}%)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQ0DC2FYIqCW","outputId":"55b955e1-858f-41cb-fc58-eec853c62ed8","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Class Distribution ===\n","HER2(+)          Count:  150   (25.82%)\n","Luminal A        Count:  158   (27.19%)\n","Luminal B        Count:  204   (35.11%)\n","Triple negative  Count:   69   (11.88%)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","\n","def is_green_artifact_lab(img, a_thresh=100, ratio_thresh=0.01):\n","    \"\"\"\n","    Identifica artefatti verdi usando il canale 'a' di LAB.\n","    \"\"\"\n","    img_arr = np.array(img)\n","    lab = cv2.cvtColor(img_arr, cv2.COLOR_RGB2LAB)\n","    A = lab[:,:,1]\n","\n","    green_pixels = (A < a_thresh)\n","    ratio = green_pixels.mean()\n","\n","    return ratio > ratio_thresh"],"metadata":{"id":"vt-c61VdLD6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","\n","contaminated = []\n","clean = []\n","\n","for fname in os.listdir(train_img_dir):\n","     if not fname.startswith(\"img_\"):\n","         continue\n","\n","     path = os.path.join(train_img_dir, fname)\n","     img = Image.open(path).convert(\"RGB\")\n","\n","     if is_green_artifact_lab(img):\n","         contaminated.append(fname)\n","     else:\n","         clean.append(fname)\n","\n","print(\"Totale immagini analizzate :\", len(clean) + len(contaminated))\n","print(\"Immagini contaminate       :\", len(contaminated))\n","print(\"Immagini pulite            :\", len(clean))"],"metadata":{"id":"D4eIgqJRLHPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def show_contaminated(samples, base_path, n=20):\n","    if len(samples) == 0:\n","         print(\"Nessuna immagine contaminata trovata.\")\n","         return\n","\n","     plt.figure(figsize=(40,20))\n","\n","     for i, fname in enumerate(samples[:n]):\n","         img = Image.open(os.path.join(base_path, fname))\n","\n","         plt.subplot(2, (n+1)//2, i+1)\n","         plt.imshow(img)\n","         plt.title(fname)\n","         plt.axis(\"off\")\n","\n","     plt.suptitle(\"Immagini contaminate rilevate dal filtro LAB\")\n","     plt.show()\n","\n","\n","show_contaminated(contaminated, train_img_dir, n=6)"],"metadata":{"collapsed":true,"id":"OZExAnW3LQIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","def delete_contaminated_samples(img_list, img_dir, labels_csv):\n","    \"\"\"\n","    Cancella automaticamente:\n","#     - img_XXXX.png\n","#     - mask_XXXX.png\n","#     - la riga corrispondente nel CSV (sample_index)\n","     \"\"\"\n","     removed_files = []\n","\n","     for fname in img_list:\n","         img_path  = os.path.join(img_dir, fname)\n","         mask_path = os.path.join(img_dir, fname.replace(\"img_\", \"mask_\"))\n","\n","         # Cancella immagine\n","         if os.path.exists(img_path):\n","             os.remove(img_path)\n","             removed_files.append(img_path)\n","\n","         # Cancella maschera associata\n","         if os.path.exists(mask_path):\n","             os.remove(mask_path)\n","             removed_files.append(mask_path)\n","\n","     print(f\"üóëÔ∏è Deleted {len(removed_files)} files (images + masks).\")\n","\n","     # --- Aggiorna il CSV ---\n","     df = pd.read_csv(labels_csv)\n","     df[\"sample_index\"] = df[\"sample_index\"].astype(str)\n","\n","     df_new = df[~df[\"sample_index\"].isin(img_list)]\n","     df_new.to_csv(labels_csv, index=False)\n","\n","     print(f\"üìÑ Remaining labels saved: {len(df_new)}\")\n","     print(\"‚úÖ Automatic cleanup completed.\")"],"metadata":{"id":"ZdQKj673LQ97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["delete_contaminated_samples(contaminated, train_img_dir, labels_file)"],"metadata":{"id":"7sc6WBI1LT1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","##  **Data Prepocessing NEW**"],"metadata":{"id":"4C8TnH-ssIPA"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import cv2\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","\n","TRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"train_data\")\n","LABELS_CSV = os.path.join(DATASET_PATH, \"train_labels.csv\")\n","\n","# ---------------------------------------------------------\n","#  LOADING FUNCTIONS\n","# ---------------------------------------------------------\n","\n","def load_rgb(path):\n","    return Image.open(path).convert(\"RGB\")\n","\n","def load_mask(path):\n","    return Image.open(path).convert(\"L\")\n","\n","\n","# ---------------------------------------------------------\n","#  MASK CLEANING (closing + dilation)\n","# ---------------------------------------------------------\n","\n","def clean_mask(mask_arr, close_size=10, dilate_size=10):\n","    \"\"\"\n","    Cleans the tumor mask via morphological closing + dilation,\n","    producing smoother, more contiguous regions suitable for centroid extraction.\n","    \"\"\"\n","    kernel_close  = np.ones((close_size, close_size), np.uint8)\n","    kernel_dilate = np.ones((dilate_size, dilate_size), np.uint8)\n","\n","    # Closing fills holes and connects nearby fragments\n","    mask_closed = cv2.morphologyEx(mask_arr, cv2.MORPH_CLOSE, kernel_close)\n","\n","    # Dilation enlarges tumor regions slightly\n","    mask_dilated = cv2.dilate(mask_closed, kernel_dilate, iterations=1)\n","\n","    return mask_dilated\n","\n","\n","# ---------------------------------------------------------\n","#  TILE EXTRACTION FROM CENTROIDS\n","# ---------------------------------------------------------\n","\n","def extract_tiles_from_centroids(img, mask, tile_size=224, min_mask_ratio=0.01):\n","    \"\"\"\n","    Extract tiles centered on connected components of the cleaned tumor mask.\n","    Produces significantly better tiles than sliding-window approaches.\n","    \"\"\"\n","    img_arr  = np.array(img)\n","    mask_arr = np.array(mask)\n","\n","    # Clean tumor regions\n","    mask_clean = clean_mask(mask_arr)\n","\n","    # Connected components extraction\n","    num_labels, labels = cv2.connectedComponents((mask_clean > 0).astype(np.uint8))\n","\n","    H, W = img_arr.shape[:2]\n","    tiles = []\n","    half = tile_size // 2\n","\n","    for lbl in range(1, num_labels):  # skip background (label 0)\n","        ys, xs = np.where(labels == lbl)\n","        if len(xs) == 0:\n","            continue\n","\n","        # Centroid of tumor region\n","        cx, cy = int(xs.mean()), int(ys.mean())\n","\n","        # Tile boundaries (centered on tumor)\n","        x1, x2 = cx - half, cx + half\n","        y1, y2 = cy - half, cy + half\n","\n","        # Padding if tile goes out of bounds\n","        pad_left   = max(0, -x1)\n","        pad_top    = max(0, -y1)\n","        pad_right  = max(0, x2 - W)\n","        pad_bottom = max(0, y2 - H)\n","\n","        tile_img = img_arr[max(y1, 0):min(y2, H), max(x1, 0):min(x2, W)]\n","        tile_mask = mask_clean[max(y1, 0):min(y2, H), max(x1, 0):min(x2, W)]\n","\n","        if pad_left or pad_top or pad_right or pad_bottom:\n","            tile_img = cv2.copyMakeBorder(tile_img, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n","            tile_mask = cv2.copyMakeBorder(tile_mask, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n","\n","        # Reject tiles with insufficient tumor content\n","        if (tile_mask > 0).mean() < min_mask_ratio:\n","            continue\n","\n","        tiles.append((Image.fromarray(tile_img), tile_mask))\n","\n","    return tiles\n","\n","\n","# ---------------------------------------------------------\n","#  OPTIONAL FALLBACK: SLIDING WINDOW (for safety)\n","# ---------------------------------------------------------\n","\n","def fallback_sliding_window(img, mask, tile_size=224, min_mask_ratio=0.01):\n","    \"\"\"\n","    A light fallback in case centroid-based extraction produces no tiles.\n","    \"\"\"\n","    img_arr  = np.array(img)\n","    mask_arr = np.array(mask)\n","\n","    H, W = img_arr.shape[:2]\n","    tiles = []\n","\n","    stride = tile_size // 2  # 50% overlap\n","\n","    for y in range(0, H - tile_size + 1, stride):\n","        for x in range(0, W - tile_size + 1, stride):\n","\n","            tile_mask = mask_arr[y:y+tile_size, x:x+tile_size]\n","            if (tile_mask > 0).mean() < min_mask_ratio:\n","                continue\n","\n","            tile_img = img_arr[y:y+tile_size, x:x+tile_size]\n","            tiles.append((Image.fromarray(tile_img), tile_mask))\n","\n","    return tiles"],"metadata":{"id":"Mepk_fPVsIPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# MULTISCALE TILE EXTRACTION VIA CENTROIDS (need also the clean_mask cells runned)\n","# ============================================================\n","\n","def extract_centered_crop(img_arr, cx, cy, crop_size):\n","    \"\"\"\n","    Extract a square crop centered at (cx, cy) with padding if needed.\n","    \"\"\"\n","    H, W = img_arr.shape[:2]\n","    half = crop_size // 2\n","\n","    x1, x2 = cx - half, cx + half\n","    y1, y2 = cy - half, cy + half\n","\n","    pad_left   = max(0, -x1)\n","    pad_top    = max(0, -y1)\n","    pad_right  = max(0, x2 - W)\n","    pad_bottom = max(0, y2 - H)\n","\n","    crop = img_arr[max(y1, 0):min(y2, H),\n","                   max(x1, 0):min(x2, W)]\n","\n","    if pad_left or pad_top or pad_right or pad_bottom:\n","        crop = cv2.copyMakeBorder(\n","            crop,\n","            pad_top, pad_bottom, pad_left, pad_right,\n","            cv2.BORDER_CONSTANT,\n","            value=0\n","        )\n","\n","    return crop\n","\n","def extract_multiscale_tiles_from_centroids(\n","    img,\n","    mask,\n","    tile_size=224,\n","    min_mask_ratio=0.01,\n","    zoom_factors=(0.58, 1.0)  # zoom-in, base, zoom-out\n","):\n","    \"\"\"\n","    Extract multiscale tiles (zoom-in / base / zoom-out) centered on tumor regions.\n","    All outputs are resized to tile_size x tile_size.\n","    \"\"\"\n","    img_arr  = np.array(img)\n","    mask_arr = np.array(mask)\n","\n","    mask_clean = clean_mask(mask_arr)\n","    num_labels, labels = cv2.connectedComponents((mask_clean > 0).astype(np.uint8))\n","\n","    H, W = img_arr.shape[:2]\n","    tiles = []\n","\n","    for lbl in range(1, num_labels):\n","        ys, xs = np.where(labels == lbl)\n","        if len(xs) == 0:\n","            continue\n","\n","        cx, cy = int(xs.mean()), int(ys.mean())\n","\n","        for zf in zoom_factors:\n","            crop_size = int(tile_size * zf)\n","\n","            # extract image + mask crop\n","            crop_img  = extract_centered_crop(img_arr,  cx, cy, crop_size)\n","            crop_mask = extract_centered_crop(mask_clean, cx, cy, crop_size)\n","\n","            # resize to 224x224\n","            crop_img  = cv2.resize(crop_img,  (tile_size, tile_size), interpolation=cv2.INTER_LINEAR)\n","            crop_mask = cv2.resize(crop_mask, (tile_size, tile_size), interpolation=cv2.INTER_NEAREST)\n","\n","            # tumor content check (on resized mask)\n","            if (crop_mask > 0).mean() < min_mask_ratio:\n","                continue\n","\n","            tiles.append((Image.fromarray(crop_img), crop_mask))\n","\n","    return tiles"],"metadata":{"id":"WbRU_cm9sIPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class HistologyOvRTiles(Dataset):\n","    \"\"\"\n","    OvR Dataset working directly on in-memory tiles.\n","    labels_df must contain:\n","      - tile_img (PIL.Image)\n","      - binary_label (0/1)\n","    \"\"\"\n","    def __init__(self, labels_df, transform=None):\n","        self.labels_df = labels_df.reset_index(drop=True)\n","        self.transform = transform\n","\n","        self.samples = [\n","            (row[\"tile_img\"], int(row[\"binary_label\"]))\n","            for _, row in self.labels_df.iterrows()\n","        ]\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img, y = self.samples[idx]\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, y"],"metadata":{"id":"wpkhi915TAz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","##  **Data Split and Data Transformation**"],"metadata":{"id":"7xxecHoWM4gk"}},{"cell_type":"code","source":["import pandas as pd\n","\n","def extract_tiles_in_memory(\n","    img_dir,\n","    slide_df,\n","    tile_size=224,\n","    min_mask_ratio=0.01,\n","    zoom_factors=(0.58, 1.0)\n","):\n","    \"\"\"\n","    Returns tiles_df with columns:\n","      sample_index | label | tile_img (PIL.Image)\n","    \"\"\"\n","    rows = []\n","\n","    for _, row in slide_df.iterrows():\n","        slide_id = row[\"sample_index\"]\n","        label    = row[\"label\"]\n","\n","        img_path  = os.path.join(img_dir, slide_id)\n","        mask_path = img_path.replace(\"img_\", \"mask_\")\n","\n","        if not os.path.exists(img_path) or not os.path.exists(mask_path):\n","            print(f\"[WARNING] Missing img/mask for {slide_id}\")\n","            continue\n","\n","        img  = load_rgb(img_path)\n","        mask = load_mask(mask_path)\n","\n","        tiles = extract_multiscale_tiles_from_centroids(\n","            img,\n","            mask,\n","            tile_size=tile_size,\n","            min_mask_ratio=min_mask_ratio,\n","            zoom_factors=zoom_factors\n","        )\n","\n","        if not tiles:\n","            tiles = fallback_sliding_window(\n","                img, mask,\n","                tile_size=tile_size,\n","                min_mask_ratio=min_mask_ratio * 0.1\n","            )\n","\n","        if not tiles:\n","            print(f\"[ERROR] No tiles for {slide_id}\")\n","            continue\n","\n","        for tile_img, _ in tiles:\n","            rows.append({\n","                \"sample_index\": slide_id,\n","                \"label\": label,\n","                \"tile_img\": tile_img\n","            })\n","\n","    tiles_df = pd.DataFrame(rows)\n","    print(f\"Total tiles extracted: {len(tiles_df)}\")\n","    return tiles_df\n","\n","def create_ovr_dataframe_tiles(tiles_df, target_class, seed=42):\n","    \"\"\"\n","    tiles_df: columns [sample_index, label, tile_img]\n","    returns tiles_df with column binary_label\n","    \"\"\"\n","    target_df = tiles_df[tiles_df[\"label\"] == target_class].copy()\n","    target_df[\"binary_label\"] = 1\n","\n","    rest_df = tiles_df[tiles_df[\"label\"] != target_class].copy()\n","    rest_classes = rest_df[\"label\"].unique()\n","\n","    target_count = len(target_df)\n","    samples_per_rest = max(1, target_count // max(1, len(rest_classes)))\n","\n","    rest_samples = []\n","    for rc in rest_classes:\n","        rc_df = rest_df[rest_df[\"label\"] == rc]\n","        n = min(samples_per_rest, len(rc_df))\n","        rest_samples.append(rc_df.sample(n=n, random_state=seed))\n","\n","    rest_balanced_df = pd.concat(rest_samples).copy()\n","    rest_balanced_df[\"binary_label\"] = 0\n","\n","    ovr_df = pd.concat([target_df, rest_balanced_df], ignore_index=True)\n","    ovr_df = ovr_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","\n","    return ovr_df\n"],"metadata":{"id":"VuIydIPX4LjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from sklearn.model_selection import train_test_split\n","\n","SEED = 42\n","\n","slide_df = labels_df[[\"sample_index\", \"label\"]].drop_duplicates().copy()\n","\n","# 70% for training OvR, 15% for validation and 15% for XGBoost\n","cnn_train_ids, temp_ids = train_test_split(\n","    slide_df[\"sample_index\"],\n","    test_size=0.30,\n","    random_state=SEED,\n","    stratify=slide_df[\"label\"]\n",")\n","\n","cnn_val_ids, xgb_slide_ids = train_test_split(\n","    temp_ids,\n","    test_size=0.50,\n","    random_state=SEED,\n","    stratify=slide_df.loc[\n","        slide_df[\"sample_index\"].isin(temp_ids), \"label\"\n","    ]\n",")\n","\n","print(\"CNN train slides:\", len(cnn_train_ids))\n","print(\"CNN val slides:\",   len(cnn_val_ids))\n","print(\"XGB slides:\",       len(xgb_slide_ids))\n","print(\"Total:\", len(cnn_train_ids) + len(cnn_val_ids) + len(xgb_slide_ids))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1KM775l36Uv","outputId":"f0beaa94-7ef9-4735-9a22-e48652217ff0","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CNN train slides: 406\n","CNN val slides: 87\n","XGB slides: 88\n","Total: 581\n"]}]},{"cell_type":"code","source":["tiles_train_df = extract_tiles_in_memory(\n","    img_dir=TRAIN_IMG_DIR,\n","    slide_df=slide_df[slide_df[\"sample_index\"].isin(cnn_train_ids)],\n","    tile_size=224\n",")\n","\n","tiles_val_df = extract_tiles_in_memory(\n","    img_dir=TRAIN_IMG_DIR,\n","    slide_df=slide_df[slide_df[\"sample_index\"].isin(cnn_val_ids)],\n","    tile_size=224\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Dm1qahc4Lmd","outputId":"b7f264bf-a62f-41a6-9b05-bb2e22108291"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total tiles extracted: 5113\n","Total tiles extracted: 988\n"]}]},{"cell_type":"code","source":["## custom 90-degree rotation function ##\n","class Random90Rotation(object):\n","    \"\"\"Rotates image randomly by 0, 90, 180, or 270 degrees.\"\"\"\n","    def __init__(self, p=0.5):\n","        self.degrees = [0, 90, 180, 270]\n","        self.p = p\n","\n","    def __call__(self, img):\n","        if random.random() < self.p:\n","            angle = random.choice(self.degrees)\n","            return img.rotate(angle, resample=Image.BICUBIC)\n","        return img\n"],"metadata":{"id":"UCRCS1VJ4lNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","IMAGENET_MEAN = [0.485, 0.456, 0.406]\n","IMAGENET_STD  = [0.229, 0.224, 0.225]\n","\n","train_transforms = transforms.Compose([\n","    Random90Rotation(p=0.75),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n","    transforms.RandomAffine(degrees=20, translate=(0.05,0.05), scale=(0.95,1.05)),\n","    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n","    transforms.ToTensor(),\n","    transforms.RandomErasing(p=0.25),\n","    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n","])\n","\n","val_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n","])\n","\n","class_names = sorted(slide_df[\"label\"].unique())\n","\n","ovr_train_datasets = {}\n","ovr_val_datasets   = {}\n","\n","for cls in class_names:\n","    print(f\"\\nPreparing OvR dataset for class: {cls}\")\n","\n","    train_ovr_df = create_ovr_dataframe_tiles(tiles_train_df, cls)\n","    val_ovr_df   = tiles_val_df.copy()\n","    val_ovr_df[\"binary_label\"] = (val_ovr_df[\"label\"] == cls).astype(int)\n","\n","    ovr_train_datasets[cls] = HistologyOvRTiles(\n","        labels_df=train_ovr_df,\n","        transform=train_transforms\n","    )\n","\n","    ovr_val_datasets[cls] = HistologyOvRTiles(\n","        labels_df=val_ovr_df,\n","        transform=val_transforms\n","    )\n","\n","    print(f\"  Train tiles: {len(ovr_train_datasets[cls])}\")\n","    print(f\"  Val tiles:   {len(ovr_val_datasets[cls])}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhl43-py4ox8","outputId":"ad662533-4e62-4449-8e48-6a854e6e05bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Preparing OvR dataset for class: HER2(+)\n","  Train tiles: 2808\n","  Val tiles:   988\n","\n","Preparing OvR dataset for class: Luminal A\n","  Train tiles: 2762\n","  Val tiles:   988\n","\n","Preparing OvR dataset for class: Luminal B\n","  Train tiles: 3525\n","  Val tiles:   988\n","\n","Preparing OvR dataset for class: Triple negative\n","  Train tiles: 1056\n","  Val tiles:   988\n"]}]},{"cell_type":"markdown","source":["##  **Data Loader Creation**"],"metadata":{"id":"_GJ9mpGJWZfo"}},{"cell_type":"code","source":["\n","\"\"\"##  **Data Loader Creation (OvR ‚Äì UPDATED)**\"\"\"\n","\n","from torch.utils.data import DataLoader\n","\n","# ---------------------------------------------------------\n","# Configuration\n","# ---------------------------------------------------------\n","BATCH_SIZE  = 64\n","NUM_WORKERS = 4\n","PIN_MEMORY = (device.type == \"cuda\")\n","\n","ovr_train_loaders = {}\n","ovr_val_loaders   = {}\n","\n","print(\"\\n--- Creating OvR DataLoaders (from in-memory datasets) ---\")\n","\n","for class_name in class_names:\n","    print(f\"Processing {class_name}...\")\n","\n","    train_dataset = ovr_train_datasets[class_name]\n","    val_dataset   = ovr_val_datasets[class_name]\n","\n","    ovr_train_loaders[class_name] = DataLoader(\n","        train_dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY,\n","        drop_last=True\n","    )\n","\n","    ovr_val_loaders[class_name] = DataLoader(\n","        val_dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=PIN_MEMORY\n","    )\n","\n","    print(f\"  -> Train tiles: {len(train_dataset)} | Val tiles: {len(val_dataset)}\")\n","\n","print(\"\\n‚úÖ All 4 OvR DataLoaders created correctly.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWIFneym4saS","outputId":"57022f27-2e98-4960-d9d5-5b9d81f00c5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Creating OvR DataLoaders (from in-memory datasets) ---\n","Processing HER2(+)...\n","  -> Train tiles: 2808 | Val tiles: 988\n","Processing Luminal A...\n","  -> Train tiles: 2762 | Val tiles: 988\n","Processing Luminal B...\n","  -> Train tiles: 3525 | Val tiles: 988\n","Processing Triple negative...\n","  -> Train tiles: 1056 | Val tiles: 988\n","\n","‚úÖ All 4 OvR DataLoaders created correctly.\n"]}]},{"cell_type":"markdown","source":["##  üßÆ **Network Parameters**"],"metadata":{"id":"Eb9DVpuR9gBd"}},{"cell_type":"code","source":["\n","\"\"\"##  üßÆ **Network Parameters**\"\"\"\n","\n","LEARNING_RATE = 5e-4\n","EPOCHS        = 200\n","PATIENCE      = 20\n","\n","DROPOUT_RATE  = 0.3\n","\n","criterion     = nn.CrossEntropyLoss()\n","criterion_val = nn.CrossEntropyLoss()\n","\n","# Print the defined parameters\n","print(\"Epochs:\", EPOCHS)\n","print(\"Batch Size:\", BATCH_SIZE)\n","print(\"Learning Rate:\", LEARNING_RATE)\n","print(\"Dropout Rate:\", DROPOUT_RATE)\n","print(\"Patience:\", PATIENCE)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4uEjFCV4tMu","outputId":"953fd9f1-7f68-4298-b42c-78bf62f48c6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epochs: 200\n","Batch Size: 64\n","Learning Rate: 0.0005\n","Dropout Rate: 0.3\n","Patience: 20\n"]}]},{"cell_type":"markdown","source":["##  üß† **Training Functions**"],"metadata":{"id":"6jpoBYs_9ihm"}},{"cell_type":"code","source":["\n","\"\"\"##  üß† **Training Functions**\"\"\"\n","\n","def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","            logits = model(inputs)\n","            loss = criterion(logits, targets)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","def validate_one_epoch(model, val_loader, criterion, device):\n","    model.eval()\n","\n","    running_loss = 0.0\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                logits = model(inputs)\n","                loss = criterion(logits, targets)\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            preds = logits.argmax(dim=1)\n","\n","            all_preds.append(preds.cpu().numpy())\n","            all_targets.append(targets.cpu().numpy())\n","\n","    y_true = np.concatenate(all_targets)\n","    y_pred = np.concatenate(all_preds)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    epoch_acc  = accuracy_score(y_true, y_pred)\n","\n","    # Binary F1 (for monitoring only, not early stopping)\n","    epoch_f1 = f1_score(y_true, y_pred)\n","\n","    return epoch_loss, epoch_acc, epoch_f1\n","\n","\n","def fit(\n","    model, train_loader, val_loader,\n","    epochs, criterion, optimizer, scaler, device,\n","    unfreeze_epoch=0, fine_tune_lr=None,\n","    patience=15,\n","    restore_best_weights=True,\n","    writer=None,\n","    verbose=1,\n","    experiment_name=\"\"\n","):\n","    history = {\n","        \"train_loss\": [],\n","        \"val_loss\": [],\n","        \"val_acc\": [],\n","        \"val_f1\": []\n","    }\n","\n","    best_val_loss = float(\"inf\")\n","    patience_counter = 0\n","\n","    for epoch in range(1, epochs + 1):\n","\n","        # Unfreeze backbone\n","        if epoch == unfreeze_epoch and unfreeze_epoch > 0:\n","            for p in model.parameters():\n","                p.requires_grad = True\n","            optimizer = torch.optim.Adam(model.parameters(), lr=fine_tune_lr)\n","\n","        train_loss = train_one_epoch(\n","            model, train_loader, criterion, optimizer, scaler, device\n","        )\n","\n","        val_loss, val_acc, val_f1 = validate_one_epoch(\n","            model, val_loader, criterion, device\n","        )\n","\n","        history[\"train_loss\"].append(train_loss)\n","        history[\"val_loss\"].append(val_loss)\n","        history[\"val_acc\"].append(val_acc)\n","        history[\"val_f1\"].append(val_f1)\n","\n","        if writer:\n","            writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n","            writer.add_scalar(\"Loss/Val\", val_loss, epoch)\n","            writer.add_scalar(\"Acc/Val\", val_acc, epoch)\n","            writer.add_scalar(\"F1/Val\", val_f1, epoch)\n","\n","        if verbose:\n","            print(f\"Epoch {epoch:03d} | \"\n","                  f\"Train Loss {train_loss:.4f} | \"\n","                  f\"Val Loss {val_loss:.4f} | \"\n","                  f\"Acc {val_acc:.4f} | F1 {val_f1:.4f}\")\n","\n","        # Early stopping on VAL LOSS\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            patience_counter = 0\n","            torch.save(model.state_dict(), f\"models/{experiment_name}.pt\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","    if restore_best_weights:\n","        model.load_state_dict(torch.load(f\"models/{experiment_name}.pt\"))\n","\n","    return model, history\n"],"metadata":{"id":"Z4mN9SEa4vGZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","##  **One vs Rest with XGBoost**"],"metadata":{"id":"0fMAuMfqTVpv"}},{"cell_type":"code","source":["\n","\"\"\"## üõ†Ô∏è **One-vs-Rest **\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.optim import Adam\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# ---------------------------------------------------------\n","# Configuration\n","# ---------------------------------------------------------\n","MODEL_BACKBONE = 'resnet18'\n","FREEZE_BACKBONE = True\n","UNFREEZE_EPOCH = 20\n","FINE_TUNE_LEARNING_RATE = 1e-5\n","NUM_OVR_CLASSES = 2\n","input_shape = (3, 224, 224)\n","\n","# ---------------------------------------------------------\n","# Binary ResNet18 (OvR)\n","# ---------------------------------------------------------\n","class BinaryResNet18(nn.Module):\n","    \"\"\"\n","    ResNet18 with pretrained weights for binary OvR classification.\n","    \"\"\"\n","    def __init__(self, dropout_rate=0.3, freeze_backbone=True):\n","        super().__init__()\n","\n","        self.backbone = torchvision.models.resnet18(\n","            weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n","        )\n","\n","        if freeze_backbone:\n","            for p in self.backbone.parameters():\n","                p.requires_grad = False\n","\n","        in_features = self.backbone.fc.in_features\n","\n","        self.backbone.fc = nn.Sequential(\n","            nn.Linear(in_features, in_features // 2),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(in_features // 2),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(in_features // 2, NUM_OVR_CLASSES)\n","        )\n","\n","    def forward(self, x):\n","        return self.backbone(x)\n"],"metadata":{"id":"41R2HFjc408E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ---------------------------------------------------------\n","# Training 4 OvR classifiers\n","# ---------------------------------------------------------\n","OVR_TRAINED_MODELS = {}\n","OVR_TRAINING_HISTORIES = {}\n","\n","print(\"\\n=======================================================\")\n","print(\"== STARTING TRAINING FOR 4 ONE-VS-REST CLASSIFIERS ==\")\n","print(\"=======================================================\")\n","\n","for i, class_name in enumerate(class_names):\n","\n","    print(f\"\\n>>>> TRAINING CLASSIFIER {i+1}/4: {class_name} vs REST <<<<\")\n","\n","    # -----------------------------------------------------\n","    # Model\n","    # -----------------------------------------------------\n","    model_ovr = BinaryResNet18(\n","        dropout_rate=DROPOUT_RATE,\n","        freeze_backbone=FREEZE_BACKBONE\n","    ).to(device)\n","\n","    # -----------------------------------------------------\n","    # Loss (STANDARD CE)\n","    # -----------------------------------------------------\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # -----------------------------------------------------\n","    # Optimizer (head only initially)\n","    # -----------------------------------------------------\n","    optimizer = Adam(\n","        filter(lambda p: p.requires_grad, model_ovr.parameters()),\n","        lr=LEARNING_RATE\n","    )\n","\n","    scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n","\n","    # -----------------------------------------------------\n","    # Logging\n","    # -----------------------------------------------------\n","    experiment_name = f\"ovr_{class_name.replace(' ', '_').replace('+', 'pos')}_{MODEL_BACKBONE}\"\n","    writer = SummaryWriter(f\"./{logs_dir}/{experiment_name}\")\n","\n","    print(f\"Starting experiment: {experiment_name}\")\n","\n","    # -----------------------------------------------------\n","    # Training\n","    # -----------------------------------------------------\n","    trained_model, history = fit(\n","        model=model_ovr,\n","        train_loader=ovr_train_loaders[class_name],\n","        val_loader=ovr_val_loaders[class_name],\n","        epochs=EPOCHS,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        scaler=scaler,\n","        device=device,\n","        writer=writer,\n","        verbose=5,\n","        experiment_name=experiment_name,\n","        patience=PATIENCE,\n","        unfreeze_epoch=UNFREEZE_EPOCH,\n","        fine_tune_lr=FINE_TUNE_LEARNING_RATE\n","    )\n","\n","    # -----------------------------------------------------\n","    # Save results\n","    # -----------------------------------------------------\n","    OVR_TRAINED_MODELS[class_name] = trained_model\n","    OVR_TRAINING_HISTORIES[class_name] = history\n","\n","    model_path = f\"models/{experiment_name}.pt\"\n","    torch.save(trained_model.state_dict(), model_path)\n","\n","    print(f\"Saved model for {class_name} ‚Üí {model_path}\")\n","    print(f\"<<<< CLASSIFIER {class_name} TRAINING COMPLETE >>>>\")\n","\n","print(\"\\n=======================================================\")\n","print(\"== ALL 4 OVR CLASSIFIERS TRAINED ==\")\n","print(\"=======================================================\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIzP4Rev45ai","outputId":"c20432d9-4ee9-4e99-9e27-2c69d68eba3d","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=======================================================\n","== STARTING TRAINING FOR 4 ONE-VS-REST CLASSIFIERS ==\n","=======================================================\n","\n",">>>> TRAINING CLASSIFIER 1/4: HER2(+) vs REST <<<<\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 243MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Starting experiment: ovr_HER2(pos)_resnet18\n","Epoch 001 | Train Loss 0.7120 | Val Loss 0.7068 | Acc 0.5698 | F1 0.3451\n","Epoch 002 | Train Loss 0.6737 | Val Loss 0.8331 | Acc 0.4555 | F1 0.4049\n","Epoch 003 | Train Loss 0.6698 | Val Loss 0.8391 | Acc 0.3957 | F1 0.3889\n","Epoch 004 | Train Loss 0.6597 | Val Loss 0.6809 | Acc 0.5962 | F1 0.3179\n","Epoch 005 | Train Loss 0.6578 | Val Loss 0.7464 | Acc 0.5111 | F1 0.3620\n","Epoch 006 | Train Loss 0.6468 | Val Loss 0.6836 | Acc 0.5860 | F1 0.3103\n","Epoch 007 | Train Loss 0.6406 | Val Loss 0.7339 | Acc 0.5071 | F1 0.3301\n","Epoch 008 | Train Loss 0.6484 | Val Loss 0.6948 | Acc 0.5749 | F1 0.2977\n","Epoch 009 | Train Loss 0.6451 | Val Loss 0.6941 | Acc 0.5536 | F1 0.3055\n","Epoch 010 | Train Loss 0.6368 | Val Loss 0.7434 | Acc 0.5091 | F1 0.3383\n","Epoch 011 | Train Loss 0.6433 | Val Loss 0.6817 | Acc 0.5769 | F1 0.3301\n","Epoch 012 | Train Loss 0.6399 | Val Loss 0.7676 | Acc 0.4889 | F1 0.4229\n","Epoch 013 | Train Loss 0.6361 | Val Loss 0.7353 | Acc 0.5061 | F1 0.3838\n","Epoch 014 | Train Loss 0.6347 | Val Loss 0.6818 | Acc 0.5860 | F1 0.3477\n","Epoch 015 | Train Loss 0.6283 | Val Loss 0.7128 | Acc 0.5395 | F1 0.3434\n","Epoch 016 | Train Loss 0.6390 | Val Loss 0.6646 | Acc 0.6002 | F1 0.3130\n","Epoch 017 | Train Loss 0.6468 | Val Loss 0.7010 | Acc 0.5415 | F1 0.3647\n","Epoch 018 | Train Loss 0.6347 | Val Loss 0.7578 | Acc 0.5172 | F1 0.3813\n","Epoch 019 | Train Loss 0.6330 | Val Loss 0.7090 | Acc 0.5486 | F1 0.3771\n","Epoch 020 | Train Loss 0.6247 | Val Loss 0.7324 | Acc 0.5192 | F1 0.3725\n","Epoch 021 | Train Loss 0.6093 | Val Loss 0.7471 | Acc 0.5071 | F1 0.3828\n","Epoch 022 | Train Loss 0.6030 | Val Loss 0.7302 | Acc 0.5324 | F1 0.3840\n","Epoch 023 | Train Loss 0.5952 | Val Loss 0.7396 | Acc 0.5273 | F1 0.3765\n","Epoch 024 | Train Loss 0.5863 | Val Loss 0.7675 | Acc 0.5121 | F1 0.3990\n","Epoch 025 | Train Loss 0.5865 | Val Loss 0.7704 | Acc 0.5051 | F1 0.3956\n","Epoch 026 | Train Loss 0.5710 | Val Loss 0.7441 | Acc 0.5466 | F1 0.4043\n","Epoch 027 | Train Loss 0.5565 | Val Loss 0.7742 | Acc 0.5314 | F1 0.4041\n","Epoch 028 | Train Loss 0.5581 | Val Loss 0.7510 | Acc 0.5648 | F1 0.4011\n","Epoch 029 | Train Loss 0.5451 | Val Loss 0.7478 | Acc 0.5658 | F1 0.4083\n","Epoch 030 | Train Loss 0.5451 | Val Loss 0.7932 | Acc 0.5395 | F1 0.4099\n","Epoch 031 | Train Loss 0.5245 | Val Loss 0.7915 | Acc 0.5557 | F1 0.4154\n","Epoch 032 | Train Loss 0.5129 | Val Loss 0.7691 | Acc 0.5749 | F1 0.4000\n","Epoch 033 | Train Loss 0.5150 | Val Loss 0.7647 | Acc 0.5820 | F1 0.3971\n","Epoch 034 | Train Loss 0.5034 | Val Loss 0.8138 | Acc 0.5597 | F1 0.4161\n","Epoch 035 | Train Loss 0.4910 | Val Loss 0.8290 | Acc 0.5577 | F1 0.4118\n","Epoch 036 | Train Loss 0.4958 | Val Loss 0.8773 | Acc 0.5314 | F1 0.4132\n","Early stopping triggered.\n","Saved model for HER2(+) ‚Üí models/ovr_HER2(pos)_resnet18.pt\n","<<<< CLASSIFIER HER2(+) TRAINING COMPLETE >>>>\n","\n",">>>> TRAINING CLASSIFIER 2/4: Luminal A vs REST <<<<\n","Starting experiment: ovr_Luminal_A_resnet18\n","Epoch 001 | Train Loss 0.6823 | Val Loss 0.6869 | Acc 0.6154 | F1 0.4025\n","Epoch 002 | Train Loss 0.6413 | Val Loss 0.7297 | Acc 0.6022 | F1 0.4889\n","Epoch 003 | Train Loss 0.6340 | Val Loss 0.7694 | Acc 0.5648 | F1 0.4612\n","Epoch 004 | Train Loss 0.6105 | Val Loss 0.6819 | Acc 0.6265 | F1 0.4825\n","Epoch 005 | Train Loss 0.6036 | Val Loss 0.6474 | Acc 0.6447 | F1 0.4706\n","Epoch 006 | Train Loss 0.6122 | Val Loss 0.7532 | Acc 0.5445 | F1 0.4539\n","Epoch 007 | Train Loss 0.6060 | Val Loss 0.6616 | Acc 0.6296 | F1 0.4665\n","Epoch 008 | Train Loss 0.5930 | Val Loss 0.7142 | Acc 0.5951 | F1 0.4695\n","Epoch 009 | Train Loss 0.6083 | Val Loss 0.6378 | Acc 0.6690 | F1 0.4818\n","Epoch 010 | Train Loss 0.6006 | Val Loss 0.6609 | Acc 0.6285 | F1 0.4611\n","Epoch 011 | Train Loss 0.6068 | Val Loss 0.8026 | Acc 0.5213 | F1 0.4494\n","Epoch 012 | Train Loss 0.5960 | Val Loss 0.6214 | Acc 0.6731 | F1 0.4696\n","Epoch 013 | Train Loss 0.5968 | Val Loss 0.7558 | Acc 0.5374 | F1 0.4514\n","Epoch 014 | Train Loss 0.6029 | Val Loss 0.6614 | Acc 0.6134 | F1 0.4543\n","Epoch 015 | Train Loss 0.5911 | Val Loss 0.7530 | Acc 0.5385 | F1 0.4506\n","Epoch 016 | Train Loss 0.6040 | Val Loss 0.7165 | Acc 0.5709 | F1 0.4508\n","Epoch 017 | Train Loss 0.5959 | Val Loss 0.6920 | Acc 0.5840 | F1 0.4423\n","Epoch 018 | Train Loss 0.5974 | Val Loss 0.7667 | Acc 0.5577 | F1 0.4664\n","Epoch 019 | Train Loss 0.5879 | Val Loss 0.6300 | Acc 0.6538 | F1 0.4689\n","Epoch 020 | Train Loss 0.5932 | Val Loss 0.6872 | Acc 0.6113 | F1 0.4667\n","Epoch 021 | Train Loss 0.5690 | Val Loss 0.6757 | Acc 0.6134 | F1 0.4665\n","Epoch 022 | Train Loss 0.5626 | Val Loss 0.6775 | Acc 0.6093 | F1 0.4624\n","Epoch 023 | Train Loss 0.5594 | Val Loss 0.6712 | Acc 0.6255 | F1 0.4638\n","Epoch 024 | Train Loss 0.5531 | Val Loss 0.6720 | Acc 0.6346 | F1 0.4699\n","Epoch 025 | Train Loss 0.5540 | Val Loss 0.6858 | Acc 0.6184 | F1 0.4652\n","Epoch 026 | Train Loss 0.5324 | Val Loss 0.6650 | Acc 0.6377 | F1 0.4704\n","Epoch 027 | Train Loss 0.5289 | Val Loss 0.6717 | Acc 0.6306 | F1 0.4640\n","Epoch 028 | Train Loss 0.5172 | Val Loss 0.6753 | Acc 0.6417 | F1 0.4732\n","Epoch 029 | Train Loss 0.5112 | Val Loss 0.6488 | Acc 0.6579 | F1 0.4652\n","Epoch 030 | Train Loss 0.4931 | Val Loss 0.6462 | Acc 0.6609 | F1 0.4535\n","Epoch 031 | Train Loss 0.4973 | Val Loss 0.6667 | Acc 0.6407 | F1 0.4479\n","Epoch 032 | Train Loss 0.4948 | Val Loss 0.6856 | Acc 0.6336 | F1 0.4396\n","Early stopping triggered.\n","Saved model for Luminal A ‚Üí models/ovr_Luminal_A_resnet18.pt\n","<<<< CLASSIFIER Luminal A TRAINING COMPLETE >>>>\n","\n",">>>> TRAINING CLASSIFIER 3/4: Luminal B vs REST <<<<\n","Starting experiment: ovr_Luminal_B_resnet18\n","Epoch 001 | Train Loss 0.7429 | Val Loss 0.6941 | Acc 0.5698 | F1 0.4558\n","Epoch 002 | Train Loss 0.6932 | Val Loss 0.7394 | Acc 0.5253 | F1 0.4489\n","Epoch 003 | Train Loss 0.6873 | Val Loss 0.7740 | Acc 0.5020 | F1 0.4928\n","Epoch 004 | Train Loss 0.6758 | Val Loss 0.7119 | Acc 0.5526 | F1 0.4675\n","Epoch 005 | Train Loss 0.6825 | Val Loss 0.7235 | Acc 0.5364 | F1 0.4561\n","Epoch 006 | Train Loss 0.6699 | Val Loss 0.6927 | Acc 0.5688 | F1 0.4538\n","Epoch 007 | Train Loss 0.6701 | Val Loss 0.7212 | Acc 0.5344 | F1 0.4773\n","Epoch 008 | Train Loss 0.6645 | Val Loss 0.7061 | Acc 0.5678 | F1 0.4447\n","Epoch 009 | Train Loss 0.6659 | Val Loss 0.7400 | Acc 0.5152 | F1 0.4624\n","Epoch 010 | Train Loss 0.6660 | Val Loss 0.7252 | Acc 0.5334 | F1 0.4695\n","Epoch 011 | Train Loss 0.6652 | Val Loss 0.7282 | Acc 0.5132 | F1 0.4685\n","Epoch 012 | Train Loss 0.6646 | Val Loss 0.7272 | Acc 0.5385 | F1 0.4597\n","Epoch 013 | Train Loss 0.6670 | Val Loss 0.6935 | Acc 0.5901 | F1 0.4475\n","Epoch 014 | Train Loss 0.6637 | Val Loss 0.8476 | Acc 0.4251 | F1 0.4947\n","Epoch 015 | Train Loss 0.6639 | Val Loss 0.7218 | Acc 0.5243 | F1 0.4635\n","Epoch 016 | Train Loss 0.6602 | Val Loss 0.7283 | Acc 0.5385 | F1 0.4479\n","Epoch 017 | Train Loss 0.6615 | Val Loss 0.7437 | Acc 0.5223 | F1 0.4744\n","Epoch 018 | Train Loss 0.6582 | Val Loss 0.7189 | Acc 0.5405 | F1 0.4634\n","Epoch 019 | Train Loss 0.6543 | Val Loss 0.7521 | Acc 0.5354 | F1 0.4860\n","Epoch 020 | Train Loss 0.6501 | Val Loss 0.7394 | Acc 0.5547 | F1 0.4884\n","Epoch 021 | Train Loss 0.6509 | Val Loss 0.7560 | Acc 0.5466 | F1 0.4874\n","Epoch 022 | Train Loss 0.6313 | Val Loss 0.7377 | Acc 0.5729 | F1 0.4988\n","Epoch 023 | Train Loss 0.6245 | Val Loss 0.7444 | Acc 0.5506 | F1 0.5000\n","Epoch 024 | Train Loss 0.6296 | Val Loss 0.7466 | Acc 0.5496 | F1 0.4983\n","Epoch 025 | Train Loss 0.6166 | Val Loss 0.7472 | Acc 0.5506 | F1 0.4873\n","Epoch 026 | Train Loss 0.6077 | Val Loss 0.7570 | Acc 0.5506 | F1 0.4932\n","Early stopping triggered.\n","Saved model for Luminal B ‚Üí models/ovr_Luminal_B_resnet18.pt\n","<<<< CLASSIFIER Luminal B TRAINING COMPLETE >>>>\n","\n",">>>> TRAINING CLASSIFIER 4/4: Triple negative vs REST <<<<\n","Starting experiment: ovr_Triple_negative_resnet18\n","Epoch 001 | Train Loss 0.7069 | Val Loss 0.7674 | Acc 0.4585 | F1 0.1857\n","Epoch 002 | Train Loss 0.6312 | Val Loss 0.7086 | Acc 0.5911 | F1 0.2016\n","Epoch 003 | Train Loss 0.6335 | Val Loss 0.5946 | Acc 0.7065 | F1 0.1989\n","Epoch 004 | Train Loss 0.6228 | Val Loss 0.6291 | Acc 0.6690 | F1 0.1886\n","Epoch 005 | Train Loss 0.6009 | Val Loss 0.6447 | Acc 0.6609 | F1 0.1728\n","Epoch 006 | Train Loss 0.6291 | Val Loss 0.5631 | Acc 0.7186 | F1 0.1524\n","Epoch 007 | Train Loss 0.6089 | Val Loss 0.6588 | Acc 0.6427 | F1 0.1959\n","Epoch 008 | Train Loss 0.5873 | Val Loss 0.6390 | Acc 0.6589 | F1 0.2323\n","Epoch 009 | Train Loss 0.5820 | Val Loss 0.5913 | Acc 0.6964 | F1 0.2228\n","Epoch 010 | Train Loss 0.5759 | Val Loss 0.5899 | Acc 0.7136 | F1 0.2028\n","Epoch 011 | Train Loss 0.5905 | Val Loss 0.4979 | Acc 0.7905 | F1 0.1551\n","Epoch 012 | Train Loss 0.5832 | Val Loss 0.5729 | Acc 0.7166 | F1 0.1617\n","Epoch 013 | Train Loss 0.5733 | Val Loss 0.6282 | Acc 0.6802 | F1 0.1856\n","Epoch 014 | Train Loss 0.5765 | Val Loss 0.5074 | Acc 0.7986 | F1 0.0995\n","Epoch 015 | Train Loss 0.5755 | Val Loss 0.6776 | Acc 0.6296 | F1 0.2009\n","Epoch 016 | Train Loss 0.5865 | Val Loss 0.5630 | Acc 0.7348 | F1 0.1812\n","Epoch 017 | Train Loss 0.5813 | Val Loss 0.6248 | Acc 0.6680 | F1 0.1633\n","Epoch 018 | Train Loss 0.5611 | Val Loss 0.6006 | Acc 0.7146 | F1 0.1989\n","Epoch 019 | Train Loss 0.5659 | Val Loss 0.6454 | Acc 0.6417 | F1 0.1767\n","Epoch 020 | Train Loss 0.5636 | Val Loss 0.6097 | Acc 0.6893 | F1 0.2108\n","Epoch 021 | Train Loss 0.5550 | Val Loss 0.6240 | Acc 0.6690 | F1 0.2044\n","Epoch 022 | Train Loss 0.5656 | Val Loss 0.5912 | Acc 0.7105 | F1 0.2056\n","Epoch 023 | Train Loss 0.5375 | Val Loss 0.5967 | Acc 0.7014 | F1 0.2175\n","Epoch 024 | Train Loss 0.5417 | Val Loss 0.5717 | Acc 0.7247 | F1 0.2139\n","Epoch 025 | Train Loss 0.5107 | Val Loss 0.5403 | Acc 0.7510 | F1 0.2166\n","Epoch 026 | Train Loss 0.5327 | Val Loss 0.5596 | Acc 0.7328 | F1 0.2326\n","Epoch 027 | Train Loss 0.5103 | Val Loss 0.5710 | Acc 0.7277 | F1 0.2380\n","Epoch 028 | Train Loss 0.5077 | Val Loss 0.5708 | Acc 0.7287 | F1 0.2299\n","Epoch 029 | Train Loss 0.4871 | Val Loss 0.5900 | Acc 0.7166 | F1 0.2432\n","Epoch 030 | Train Loss 0.5060 | Val Loss 0.5949 | Acc 0.7095 | F1 0.2306\n","Epoch 031 | Train Loss 0.4842 | Val Loss 0.5431 | Acc 0.7642 | F1 0.2410\n","Early stopping triggered.\n","Saved model for Triple negative ‚Üí models/ovr_Triple_negative_resnet18.pt\n","<<<< CLASSIFIER Triple negative TRAINING COMPLETE >>>>\n","\n","=======================================================\n","== ALL 4 OVR CLASSIFIERS TRAINED ==\n","=======================================================\n"]}]},{"cell_type":"code","source":["\n","\"\"\" XGboost new \"\"\"\n","# ---------------------------------------------------------\n","# XGBoost WSI-level split\n","# ---------------------------------------------------------\n","\n","\n","xgb_slide_df = slide_df[slide_df[\"sample_index\"].isin(xgb_slide_ids)]\n","\n","print(f\"XGBoost slides: {len(xgb_slide_df)}\")\n","\n","# ---------------------------------------------------------\n","# Extract tiles for XGBoost WSIs (Without overlap with CNN)\n","# ---------------------------------------------------------\n","tiles_xgb_df = extract_tiles_in_memory(\n","    img_dir=TRAIN_IMG_DIR,\n","    slide_df=xgb_slide_df,\n","    tile_size=224\n",")\n","\n","print(f\"Total XGBoost tiles extracted: {len(tiles_xgb_df)}\")\n","\n","assert len(\n","    set(tiles_xgb_df[\"sample_index\"]) &\n","    set(tiles_val_df[\"sample_index\"])\n",") == 0, \"‚ùå DATA LEAKAGE: XGB tiles overlap CNN validation tiles\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vX-G0DMx477O","outputId":"7008ed9f-8b09-4e20-b31e-6b40df917dc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["XGBoost slides: 88\n","Total tiles extracted: 988\n","Total XGBoost tiles extracted: 988\n"]}]},{"cell_type":"code","source":["\n","import torch.nn.functional as F\n","\n","def infer_tiles_ovr(model, tiles_df, transform, device):\n","    \"\"\"\n","    tiles_df: columns [sample_index, tile_img]\n","    Returns: dict {sample_index: np.array(probabilities)}\n","    \"\"\"\n","    model.eval()\n","    slide_probs = {}\n","\n","    with torch.no_grad():\n","        for slide_id, group in tiles_df.groupby(\"sample_index\"):\n","            probs = []\n","\n","            for img in group[\"tile_img\"]:\n","                x = transform(img).unsqueeze(0).to(device)\n","\n","                with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                    logits = model(x)\n","                    p = F.softmax(logits, dim=1)[0, 1].item()  # prob positive\n","\n","                probs.append(p)\n","\n","            slide_probs[slide_id] = np.array(probs)\n","\n","    return slide_probs\n","\n","from sklearn.metrics import precision_recall_curve\n","\n","def compute_pr_threshold(y_true, y_scores):\n","    \"\"\"\n","    Returns threshold that maximizes F1 (PR-based).\n","    \"\"\"\n","    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n","    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n","    best_idx = np.argmax(f1)\n","    return thresholds[best_idx]\n"],"metadata":{"id":"8Ky5Rwus4-OJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ---------------------------------------------------------\n","# Compute thresholds œÑ_c for each OvR\n","# ---------------------------------------------------------\n","\n","ovr_thresholds = {}\n","\n","for cls in class_names:\n","    print(f\"\\nComputing PR threshold for class {cls}\")\n","\n","    model = OVR_TRAINED_MODELS[cls]\n","\n","    slide_probs = infer_tiles_ovr(\n","        model,\n","        tiles_val_df,\n","        val_transforms,\n","        device\n","    )\n","\n","    y_true = []\n","    y_score = []\n","\n","    for slide_id, probs in slide_probs.items():\n","        label = slide_df.loc[slide_df[\"sample_index\"] == slide_id, \"label\"].values[0]\n","        y_true.extend([1 if label == cls else 0] * len(probs))\n","        y_score.extend(probs.tolist())\n","\n","    tau = compute_pr_threshold(np.array(y_true), np.array(y_score))\n","    ovr_thresholds[cls] = tau\n","\n","    print(f\"  œÑ_{cls} = {tau:.4f}\")\n","\n","def build_xgb_features(slide_ids, tiles_df, ovr_models, ovr_thresholds, transform, device):\n","    \"\"\"\n","    Returns:\n","      X: np.ndarray [n_slides, 8]\n","      y: np.ndarray [n_slides]\n","    \"\"\"\n","    X, y = [], []\n","\n","    for slide_id in slide_ids:\n","        slide_tiles = tiles_df[tiles_df[\"sample_index\"] == slide_id]\n","\n","        features = []\n","\n","        for cls in class_names:\n","            model = ovr_models[cls]\n","            tau   = ovr_thresholds[cls]\n","\n","            probs = []\n","            with torch.no_grad():\n","                for img in slide_tiles[\"tile_img\"]:\n","                    x = transform(img).unsqueeze(0).to(device)\n","                    with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                        p = F.softmax(model(x), dim=1)[0, 1].item()\n","                    probs.append(p)\n","\n","            probs = np.array(probs)\n","            features.extend([\n","                np.sum(probs >= tau),\n","                np.sum(probs <  tau)\n","            ])\n","\n","        X.append(features)\n","        label = slide_df.loc[slide_df[\"sample_index\"] == slide_id, \"label\"].values[0]\n","        y.append(label)\n","\n","    return np.array(X), np.array(y)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Cr1eqgY5E4K","outputId":"02fadc5f-0d9c-428a-ec25-b82173732f6b","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Computing PR threshold for class HER2(+)\n","  œÑ_HER2(+) = 0.1669\n","\n","Computing PR threshold for class Luminal A\n","  œÑ_Luminal A = 0.4395\n","\n","Computing PR threshold for class Luminal B\n","  œÑ_Luminal B = 0.1357\n","\n","Computing PR threshold for class Triple negative\n","  œÑ_Triple negative = 0.3996\n"]}]},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","import xgboost as xgb\n","\n","# Encode labels\n","le_xgb = LabelEncoder()\n","y_encoded = le_xgb.fit_transform(xgb_slide_df[\"label\"])\n","\n","X_xgb, y_xgb = build_xgb_features(\n","    slide_ids=xgb_slide_df[\"sample_index\"].values,\n","    tiles_df=tiles_xgb_df,   # tiles from CNN-val / held-out\n","    ovr_models=OVR_TRAINED_MODELS,\n","    ovr_thresholds=ovr_thresholds,\n","    transform=val_transforms,\n","    device=device\n",")\n","\n","# Train / Val split for XGB\n","X_tr, X_va, y_tr, y_va = train_test_split(\n","    X_xgb, y_encoded,\n","    test_size=0.2,\n","    random_state=SEED,\n","    stratify=y_encoded\n",")\n","\n","# ---------------------------------------------------------\n","# XGBoost model\n","# ---------------------------------------------------------\n","dtrain = xgb.DMatrix(X_tr, label=y_tr)\n","dval   = xgb.DMatrix(X_va, label=y_va)\n","\n","params = {\n","    \"objective\": \"multi:softprob\",\n","    \"num_class\": len(class_names),\n","    \"max_depth\": 2,\n","    \"eta\": 0.03,\n","    \"subsample\": 0.8,\n","    \"colsample_bytree\": 0.8,\n","    \"eval_metric\": \"mlogloss\",\n","    \"seed\": SEED\n","}\n","\n","evals = [(dtrain, \"train\"), (dval, \"val\")]\n","\n","xgb_model = xgb.train(\n","    params=params,\n","    dtrain=dtrain,\n","    num_boost_round=200,\n","    evals=evals,\n","    early_stopping_rounds=20,\n","    verbose_eval=True\n",")\n","\n","\n","y_pred_proba = xgb_model.predict(dval)\n","y_pred = y_pred_proba.argmax(axis=1)\n","\n","print(classification_report(y_va, y_pred, target_names=le_xgb.classes_))\n","print(confusion_matrix(y_va, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"yu_t9QjG5InL","outputId":"ff939b8e-1e26-461a-fe67-1ecc7368c9df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\ttrain-mlogloss:1.33861\tval-mlogloss:1.35753\n","[1]\ttrain-mlogloss:1.32809\tval-mlogloss:1.35950\n","[2]\ttrain-mlogloss:1.31843\tval-mlogloss:1.35974\n","[3]\ttrain-mlogloss:1.30858\tval-mlogloss:1.36079\n","[4]\ttrain-mlogloss:1.29988\tval-mlogloss:1.35264\n","[5]\ttrain-mlogloss:1.28942\tval-mlogloss:1.35158\n","[6]\ttrain-mlogloss:1.28034\tval-mlogloss:1.35264\n","[7]\ttrain-mlogloss:1.27118\tval-mlogloss:1.35341\n","[8]\ttrain-mlogloss:1.26036\tval-mlogloss:1.35072\n","[9]\ttrain-mlogloss:1.25337\tval-mlogloss:1.35040\n","[10]\ttrain-mlogloss:1.24566\tval-mlogloss:1.34817\n","[11]\ttrain-mlogloss:1.23740\tval-mlogloss:1.34946\n","[12]\ttrain-mlogloss:1.23046\tval-mlogloss:1.35196\n","[13]\ttrain-mlogloss:1.22159\tval-mlogloss:1.34532\n","[14]\ttrain-mlogloss:1.21443\tval-mlogloss:1.34430\n","[15]\ttrain-mlogloss:1.20616\tval-mlogloss:1.34476\n","[16]\ttrain-mlogloss:1.19815\tval-mlogloss:1.34282\n","[17]\ttrain-mlogloss:1.19106\tval-mlogloss:1.34186\n","[18]\ttrain-mlogloss:1.18394\tval-mlogloss:1.34131\n","[19]\ttrain-mlogloss:1.17619\tval-mlogloss:1.33980\n","[20]\ttrain-mlogloss:1.16973\tval-mlogloss:1.33818\n","[21]\ttrain-mlogloss:1.16249\tval-mlogloss:1.33627\n","[22]\ttrain-mlogloss:1.15679\tval-mlogloss:1.33798\n","[23]\ttrain-mlogloss:1.15053\tval-mlogloss:1.33827\n","[24]\ttrain-mlogloss:1.14555\tval-mlogloss:1.33959\n","[25]\ttrain-mlogloss:1.13874\tval-mlogloss:1.33842\n","[26]\ttrain-mlogloss:1.13235\tval-mlogloss:1.33708\n","[27]\ttrain-mlogloss:1.12548\tval-mlogloss:1.33460\n","[28]\ttrain-mlogloss:1.11873\tval-mlogloss:1.33281\n","[29]\ttrain-mlogloss:1.11232\tval-mlogloss:1.33273\n","[30]\ttrain-mlogloss:1.10612\tval-mlogloss:1.33136\n","[31]\ttrain-mlogloss:1.10020\tval-mlogloss:1.33005\n","[32]\ttrain-mlogloss:1.09487\tval-mlogloss:1.32815\n","[33]\ttrain-mlogloss:1.08869\tval-mlogloss:1.32858\n","[34]\ttrain-mlogloss:1.08366\tval-mlogloss:1.33030\n","[35]\ttrain-mlogloss:1.07772\tval-mlogloss:1.33034\n","[36]\ttrain-mlogloss:1.07204\tval-mlogloss:1.33260\n","[37]\ttrain-mlogloss:1.06691\tval-mlogloss:1.33161\n","[38]\ttrain-mlogloss:1.06203\tval-mlogloss:1.32971\n","[39]\ttrain-mlogloss:1.05692\tval-mlogloss:1.32734\n","[40]\ttrain-mlogloss:1.05084\tval-mlogloss:1.32709\n","[41]\ttrain-mlogloss:1.04684\tval-mlogloss:1.32564\n","[42]\ttrain-mlogloss:1.04345\tval-mlogloss:1.32652\n","[43]\ttrain-mlogloss:1.03913\tval-mlogloss:1.32508\n","[44]\ttrain-mlogloss:1.03471\tval-mlogloss:1.32412\n","[45]\ttrain-mlogloss:1.03064\tval-mlogloss:1.32345\n","[46]\ttrain-mlogloss:1.02648\tval-mlogloss:1.32616\n","[47]\ttrain-mlogloss:1.02262\tval-mlogloss:1.32706\n","[48]\ttrain-mlogloss:1.01769\tval-mlogloss:1.32949\n","[49]\ttrain-mlogloss:1.01388\tval-mlogloss:1.33100\n","[50]\ttrain-mlogloss:1.00958\tval-mlogloss:1.33139\n","[51]\ttrain-mlogloss:1.00518\tval-mlogloss:1.33156\n","[52]\ttrain-mlogloss:1.00071\tval-mlogloss:1.33178\n","[53]\ttrain-mlogloss:0.99639\tval-mlogloss:1.32883\n","[54]\ttrain-mlogloss:0.99220\tval-mlogloss:1.32631\n","[55]\ttrain-mlogloss:0.98759\tval-mlogloss:1.32634\n","[56]\ttrain-mlogloss:0.98420\tval-mlogloss:1.32602\n","[57]\ttrain-mlogloss:0.97926\tval-mlogloss:1.32762\n","[58]\ttrain-mlogloss:0.97488\tval-mlogloss:1.32683\n","[59]\ttrain-mlogloss:0.97185\tval-mlogloss:1.32718\n","[60]\ttrain-mlogloss:0.96790\tval-mlogloss:1.32680\n","[61]\ttrain-mlogloss:0.96338\tval-mlogloss:1.32677\n","[62]\ttrain-mlogloss:0.96058\tval-mlogloss:1.32811\n","[63]\ttrain-mlogloss:0.95821\tval-mlogloss:1.32589\n","[64]\ttrain-mlogloss:0.95513\tval-mlogloss:1.32515\n","[65]\ttrain-mlogloss:0.95195\tval-mlogloss:1.32443\n","                 precision    recall  f1-score   support\n","\n","        HER2(+)       0.33      0.40      0.36         5\n","      Luminal A       0.40      0.40      0.40         5\n","      Luminal B       0.14      0.17      0.15         6\n","Triple negative       0.00      0.00      0.00         2\n","\n","       accuracy                           0.28        18\n","      macro avg       0.22      0.24      0.23        18\n","   weighted avg       0.25      0.28      0.26        18\n","\n","[[2 1 2 0]\n"," [0 2 3 0]\n"," [3 2 1 0]\n"," [1 0 1 0]]\n"]}]},{"cell_type":"markdown","source":["## **Inference**"],"metadata":{"id":"aQ41S9bBjv8K"}},{"cell_type":"code","source":["test_slide_ids = sorted(\n","    f for f in os.listdir(test_img_dir)\n","    if f.startswith(\"img_\")\n",")\n","\n","print(f\"Test slides: {len(test_slide_ids)}\")\n","\n","test_slide_df = pd.DataFrame({\n","    \"sample_index\": test_slide_ids,\n","    \"label\": \"unknown\" # Add a dummy label for the test set\n","})\n","\n","tiles_test_df = extract_tiles_in_memory(\n","    img_dir=test_img_dir,\n","    slide_df=test_slide_df,\n","    tile_size=224\n",")\n","\n","print(f\"Total test tiles extracted: {len(tiles_test_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCxXSs2EA-jR","outputId":"3a00b4e3-f16c-4dc9-b52d-fbef9b33f931"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test slides: 477\n","Total tiles extracted: 5533\n","Total test tiles extracted: 5533\n"]}]},{"cell_type":"code","source":["def infer_tiles_ovr_test(model, tiles_df, transform, device):\n","    model.eval()\n","    slide_probs = {}\n","\n","    with torch.no_grad():\n","        for slide_id, group in tiles_df.groupby(\"sample_index\"):\n","            probs = []\n","\n","            for img in group[\"tile_img\"]:\n","                x = transform(img).unsqueeze(0).to(device)\n","\n","                with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                    p = torch.softmax(model(x), dim=1)[0, 1].item()\n","\n","                probs.append(p)\n","\n","            slide_probs[slide_id] = np.array(probs)\n","\n","    return slide_probs\n"],"metadata":{"id":"4HIBlL7vBFLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_xgb_features_test(\n","    slide_ids, tiles_df,\n","    ovr_models, ovr_thresholds,\n","    transform, device\n","):\n","    X = []\n","\n","    for slide_id in slide_ids:\n","        slide_tiles = tiles_df[tiles_df[\"sample_index\"] == slide_id]\n","\n","        features = []\n","\n","        for cls in class_names:\n","            model = ovr_models[cls]\n","            tau   = ovr_thresholds[cls]\n","\n","            probs = []\n","            with torch.no_grad():\n","                for img in slide_tiles[\"tile_img\"]:\n","                    x = transform(img).unsqueeze(0).to(device)\n","                    with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                        p = torch.softmax(model(x), dim=1)[0, 1].item()\n","                    probs.append(p)\n","\n","            probs = np.array(probs)\n","\n","            # SAME FEATURES AS TRAINING\n","            features.extend([\n","                np.sum(probs >= tau),\n","                np.sum(probs <  tau)\n","            ])\n","\n","        X.append(features)\n","\n","    return np.array(X)\n"],"metadata":{"id":"kONtpdCdBGa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","test_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n","])\n","\n","X_test = build_xgb_features_test(\n","    slide_ids=test_slide_ids,\n","    tiles_df=tiles_test_df,\n","    ovr_models=OVR_TRAINED_MODELS,\n","    ovr_thresholds=ovr_thresholds,\n","    transform=test_transforms,\n","    device=device\n",")\n"],"metadata":{"id":"SVQdG-KtBLsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import xgboost as xgb\n","\n","dtest = xgb.DMatrix(X_test)\n","\n","y_test_proba = xgb_model.predict(dtest)\n","y_test_pred = y_test_proba.argmax(axis=1)\n","\n","y_test_labels = le_xgb.inverse_transform(y_test_pred)\n","\n","submission_df = pd.DataFrame({\n","    \"sample_index\": test_slide_ids,\n","    \"predicted_label\": y_test_labels\n","})\n","\n","submission_df.to_csv(\"oneVSrest.csv\", index=False)\n","print(submission_df.head())\n","\n","from google.colab import files\n","files.download(\"oneVSrest.csv\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Hu4jqxPBNu3","outputId":"ad3b9d0e-50ad-4152-a8fc-b1e3b836ccbb","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["   sample_index predicted_label\n","0  img_0000.png       Luminal A\n","1  img_0001.png         HER2(+)\n","2  img_0002.png       Luminal B\n","3  img_0003.png         HER2(+)\n","4  img_0004.png       Luminal B\n"]}]}]}